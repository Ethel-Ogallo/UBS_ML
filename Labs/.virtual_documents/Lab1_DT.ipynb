





#conda install -c conda-forge scikit-learn, pandas 


#conda install seaborn








import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


# (1) 
data = pd.read_csv('penguins.csv')

print('dimensions:',data.shape)
data.head()


# summarry stats
data.describe()


# (2) 
sns.pairplot(data, hue='species')
plt.show() 


# (3) 
data_subset = data[['bill_length_mm','bill_depth_mm', 'species']]

Y = data['species']
X = data.drop('species', axis=1)



# (4) 
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=1023)





##TODO
from sklearn.tree import DecisionTreeClassifier

tree_clf = DecisionTreeClassifier(max_depth=2, random_state=1023)
tree_clf.fit(X_train, y_train)














from sklearn import tree

fig = plt.figure(figsize=(6, 8))
tree_plot = tree.plot_tree(tree_clf, feature_names=X.columns, class_names=tree_clf.classes_, filled=True)
plt.show()








##TODO






##------- Complete the command below by filling in the gaps '...'.-------##

range_features = {
    feature_name: (data[feature_name].min() - 1, data[feature_name].max() + 1)
    for feature_name in data.columns
}


import numpy as np

# A function to plot the partition associated to a decision tree model
def plot_decision_function(fitted_classifier, range_features, ax=None):
    """Plot the boundary of the decision function of a classifier."""
    from sklearn.preprocessing import LabelEncoder

    feature_names = list(range_features.keys())
    # create a grid to evaluate all possible samples
    plot_step = 0.02
    xx, yy = np.meshgrid(
        np.arange(*range_features[feature_names[0]], plot_step),
        np.arange(*range_features[feature_names[1]], plot_step),
    )

    # compute the associated prediction
    Z = fitted_classifier.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = LabelEncoder().fit_transform(Z)
    Z = Z.reshape(xx.shape)

    # make the plot of the boundary and the data samples
    if ax is None:
        _, ax = plt.subplots()
    ax.contourf(xx, yy, Z, alpha=0.4, cmap="Pastel2")

    return ax

# Define the colors used
palette = ["tab:orange", "tab:cyan", "tab:purple"]

ax = sns.scatterplot(data=...., x=....., y=......,hue=......, palette=palette)# Define the grid : the data space 
plot_decision_function(fitted_classifier=...., ax=....) # display the tree partition
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left') # graphical options
plt.title("Penguins dataset")# graphical options









##TODO





##TODO





##TODO






## Step 1
from sklearn.tree import DecisionTreeClassifier, plot_tree
max_tree_clf = DecisionTreeClassifier(random_state=2310)


## Step 2
path = max_tree_clf.cost_complexity_pruning_path(data_train, target_train)
ccp_alphas= path.ccp_alphas; #print('ccp-alphas values:',ccp_alphas)

tree_clfs = []
for alpha in ccp_alphas:
    tree_clf = DecisionTreeClassifier(random_state=2310, ccp_alpha=alpha)
    tree_clf.fit(data_train, target_train)
    tree_clfs.append(tree_clf)


tree_clfs = tree_clfs[:-1] #a list including all the decision trees for all alpha values
ccp_alphas = ccp_alphas[:-1] #a list including all alpha values


node_counts = [tree_clf.tree_.node_count for tree_clf in tree_clfs]#a vector with the number of nodes for all decision trees 
depth = [tree_clf.tree_.max_depth for tree_clf in tree_clfs]#a vector with the depth for all decision trees 


## Step 3
train_acc = [tree_clf.score(data_train, target_train) for tree_clf in tree_clfs]
test_acc = [tree_clf.score(data_test, target_test) for tree_clf in tree_clfs]


## Step 4

fig, ax = plt.subplots(figsize=(8,5))
ax.plot(ccp_alphas, node_counts, marker='o', drawstyle="steps-post")
ax.set_xlabel("alpha")
ax.set_ylabel("number of nodes")
ax.set_title("Number of nodes vs alpha")
fig.tight_layout()

fig, ax = plt.subplots(figsize=(8,5))
ax.plot(ccp_alphas, depth, marker='o', drawstyle="steps-post")
ax.set_xlabel("alpha")
ax.set_ylabel("depth of tree")
ax.set_title("Depth vs alpha")
fig.tight_layout()

fig, ax = plt.subplots(figsize=(10,6))
ax.plot(ccp_alphas, train_acc, marker='o', label="train", drawstyle="steps-post")
ax.plot(ccp_alphas, test_acc, marker='o', label="test",   drawstyle="steps-post")
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and testing sets")
ax.legend()
plt.show()





##TODO








## TODO






##TODO











##TODO







##TODO






##TODO
